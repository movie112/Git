{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paper_transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsERYGqSMl85GILHJwfB2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/movie112/Git/blob/main/210915_210928/paper_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AnaOVmaQLDT"
      },
      "source": [
        "- transformer 기반\n",
        "  - GPT: transformer의 decoder 활용\n",
        "  - BERT: transformer의 encoder 활용\n",
        "\n",
        "- seq2seq 모델에 attention 사용\n",
        "  - 디코더는 인코더의 모든 출력(output)을 참고\n",
        "  - 출력 값들을 전부 고려한 weighted sum vector를 구하고 입력으로 넣어줘서 source 전체 고려\n",
        "\n",
        "  - 디코더는 매번 인코더의 모든 출력 중에서 어떤 정보가 중요한지 계산\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAH92r_SWY68"
      },
      "source": [
        "- transformer\n",
        "  - RNN, CNN을 전혀 사용하지 않고 **positional encoding** 사용\n",
        "  - 인코더 디코더로 구성\n",
        "    - attention 과정을 여러 레이어에서 반복\n",
        "- 인코더\n",
        "  - RNN을 사용하지 않으려면 위치 정보를 포함하고 있는 임베딩을 사용해야 함 \n",
        "  - embedding이 끝난 후 attention 진행(input: 입력값+위치정보)\n",
        "  - 각각의 단어가 서로 어떤 연관을 가지는지 정보를 학습\n",
        "  - + 잔여 학습(residual learning) 사용\n",
        "  - 어텐션과 정규화 과정 반복\n",
        "    - 각 레이어는 서로 다른 파라미터 가짐\n",
        "\n",
        "- 마지막 인코더 레이어의 출력이 모든 디코더 레이어에 입력\n",
        "- 인코더 디코더를 다수 사용한다는 점이 특징\n",
        "  - <eos>가 나올 때까지 이코더를 이용\n",
        "  - myti-head attention 레이어 사용\n",
        "    - attention을 위한 세 가지 입력 요소\n",
        "    - 쿼리(query)\n",
        "    - 키(key)\n",
        "    - 값(value)\n",
        "  - 행렬 곱셈 연산을 이용해서 한꺼번에 연산 가능\n",
        "  - mask matrix를 이용해 특정 단어는 무시할 수 있도록 합니다.\n",
        "  - 마스크 값으로 음수 무한의 값을 넣어 softmax함수의 출력이 0%에 가까워지도록 함\n",
        "  - MultiHead(Q, K, V)를 수행한 뒤에도 차원(dimension)이 동일하게 유지\n",
        "  - attention layer 3가지 종류\n",
        "    - encoder self-attention\n",
        "    - masked decoder self-attention\n",
        "    - encoder-decoder attention\n",
        "\n",
        "  - self-attention은 인코더와 디코더 모두에서 사용됨\n",
        "    - 매번 입력 문장에서 각 단어가 다른 어떤 단어와 연관성이 높은 지 계산 가능\n",
        "  \n",
        "  - positional encoding\n",
        "    - BERT와 같은 향상된 네트워크에서도 채택되고 있음\n",
        "    - 인코더와 디코더로 구성\n",
        "    - attention 과정을 여러 레이어에서 반복\n",
        "\n",
        "    - positional encoding은 주기함수를 활용한 공식 사용\n",
        "      - 각 단어의 상대적인 위치정보를 네트워크에게 입력 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}